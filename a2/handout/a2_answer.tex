\documentclass[a4paper]{article}

\usepackage{amsmath, graphicx, float, blindtext} % for dummy text
\graphicspath{ {./images/} }
\title{Assignment 2: word2vec solutions}
\author{Shubham Gupta}

\begin{document}
\maketitle
\section{Understanding word2vec}
\subsection{Derivation}
From the question, we know the following:

\[
    J(\nu_c, o, \textbf{U}) = -logP(O=o | C=c) 
.\] 
We know cross entropy loss is given as:

\[
    -\sum_{w \epsilon Vocab} y_wlog(\hat{y}_w) = -log(\hat{y}_o)
.\] 
Since \textbf{y} is a one hot vector, it implies that y will be equal to 1 only when $i == o$. Hence, the LHS can be written as:

\begin{equation}
\begin{split}
& - \sum_{w \epsilon Vocab} y_wlog(\hat{y}_w) \\
& = -[y_0log(y_0) + y_1log(y_1)+...+y_olog(y_o)+...+y_wlog(y_w)] \\
& = -y_olog(y_o) \\
& = -log(y_o) \\
& = -log(P(O=o|C=c))
\end{split}
\end{equation}

\subsection{Derivation of $J_ {\text{naive softmax}}$ wrt $\nu_c$}
\begin{equation}
\begin{split}
    & CE(y, \hat{y})\\
    & \hat{y} = softmax(\theta) \frac{\partial J}{\partial \theta} = (\hat{y} - y)^T \\
\end{split}
\end{equation}
Using chain rule, we can solve this as follows:
\begin{equation}
\begin{split}
    \begin{aligned} \frac{\partial J}{\partial v_c} &= \frac{\partial J}{\partial \theta} \frac{\partial \theta}{\partial v_c} \ &= (\hat{y} - y) \frac{\partial U^Tv_c}{\partial v_c} \ &= U^T(\hat{y} - y)^T \end{aligned}
\end{split}
\end{equation}

\subsection{Derivation of $J_ {\text{naive softmax}}$ wrt outside vectors $u_w$}

\end{document}
