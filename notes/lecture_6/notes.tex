\documentclass[a4paper]{article}

\usepackage{amsmath, graphicx, float, blindtext} % for dummy text
\graphicspath{ {./images/} }
\title{Language Models and RNN}
\author{Shubham Gupta}

\begin{document}
\maketitle
\section{Introduction}
\begin{itemize}
    \item Introduce language modelling(LM)
    \item LM motivates building RNN
\end{itemize}
\subsection{Language Modelling}
\begin{itemize}
    \item Predict what words come next
    \item Computes probability distribution of next work $x^{t+1}$ given previous words $x^1,x^2...x^t$  where $x$ is in the vocab $V$.
    \item Assign prob to a piece of text
    \begin{equation}
    \begin{split}
        \prod_{t=1}^{T} P(x^t | x(t-1),...,x^1)
    \end{split}
    \end{equation}
\end{itemize}
\subsection{N-gram language model}
\begin{itemize}
    \item \textbf{n-gram} Chunk of n consecutive words 
    \item Collect stats about how frequent different n-grams are, and use this to predict the next word.
    \item \textbf{Assumption}: $x^{t+1}$ depends only on preceding $n-1$ words 
    \item To get the probabilities, we will count them from a large corpus i.e \textit{statistical approximation} 
\end{itemize}
\subsection{Problems}
\begin{itemize}
    \item Throws away too much context
    \item Sparsity. If the numerator is 0, then the chance of a valid word occuring is not possible, which is incorrect. Just because the n-words were not seen in the dataset does not mean that it is not a valid concept
    \item Solution? Smoothing. Add a small amount of probability $delta$ for every word in the vocabulary
    \item If denominator is 0, cannot calculate the probability at all.
    \item Solution? If you cannot find n words in the dataset, backoff and just use the last n-1 or n-2 words instead. This is called \textbf{backoff}  
    \item Sparsity problems increase with increase in $n$
    \item \textbf{Storage}: Size of model increases as the n-grams increase
\end{itemize}

\section{Fixed window neural language model}
\begin{itemize}
    \item Use neural network for word prediction
    \item Represent window of words as a one-hot encoding
    \item For each word, obtain the word embedding from a model such as word2vec
    \item Pass this to a hidden layer and multiply it with weight matrix containing non-linearity and a smaller size
    \item Pass output from hidden layer to softmax to get probabilities. Softmax output will be of entire vocab size i.e $V$.
    \item \textbf{Advantages} 
    \begin{itemize}
        \item No sparsity problem
        \item Don't need to store all n-grams you've seen.
    \end{itemize}
    \item \textbf{Problems} 
    \begin{itemize}
        \item Fixed window will always be small. Increasing it will increase size of $W$, leading to more problems
        \item No symmetry i.e because of matrix multiplication, each word vector is multiplied only by specified column of wight vectors. So you are learning something specific for each word rather than learning a general function.
        \item Need neural network to process strings of arbitary length
    \end{itemize}
\end{itemize}
\section{RNN}
\begin{itemize}
    \item Recurrent neural network
\end{itemize}

\end{document}
