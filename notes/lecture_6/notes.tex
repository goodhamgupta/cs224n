\documentclass[a4paper]{article}

\usepackage{amsmath, graphicx, float, blindtext} % for dummy text
\graphicspath{ {./images/} }
\title{Language Models and RNN}
\author{Shubham Gupta}

\begin{document}
\maketitle
\section{Introduction}
\begin{itemize}
    \item Introduce language modelling(LM)
    \item LM motivates building RNN
\end{itemize}
\subsection{Language Modelling}
\begin{itemize}
    \item Predict what words come next
    \item Computes probability distribution of next work $x^{t+1}$ given previous words $x^1,x^2...x^t$  where $x$ is in the vocab $V$.
    \item Assign prob to a piece of text
    \begin{equation}
    \begin{split}
        \prod_{t=1}^{T} P(x^t | x(t-1),...,x^1)
    \end{split}
    \end{equation}
\end{itemize}
\subsection{N-gram language model}
\begin{itemize}
    \item \textbf{n-gram} Chunk of n consecutive words 
    \item Collect stats about how frequent different n-grams are, and use this to predict the next word.
    \item \textbf{Assumption}: $x^{t+1}$ depends only on preceding $n-1$ words 
    \item To get the probabilities, we will count them from a large corpus i.e \textit{statistical approximation} 
\end{itemize}
\subsection{Problems}
\begin{itemize}
    \item Throws away too much context
    \item Sparsity. If the numerator is 0, then the chance of a valid word occuring is not possible, which is incorrect. Just because the n-words were not seen in the dataset does not mean that it is not a valid concept
    \item Solution? Smoothing. Add a small amount of probability $delta$ for every word in the vocabulary
    \item If denominator is 0, cannot calculate the probability at all.
    \item Solution? If you cannot find n words in the dataset, backoff and just use the last n-1 or n-2 words instead. This is called \textbf{backoff}  
    \item Sparsity problems increase with increase in $n$
    \item \textbf{Storage}: Size of model increases as the n-grams increase
\end{itemize}

\section{Fixed window neural language model}
\begin{itemize}
    \item Use neural network for word prediction
    \item Represent window of words as a one-hot encoding
    \item For each word, obtain the word embedding from a model such as word2vec
    \item Pass this to a hidden layer and multiply it with weight matrix containing non-linearity and a smaller size
    \item Pass output from hidden layer to softmax to get probabilities. Softmax output will be of entire vocab size i.e $V$.
    \item \textbf{Advantages} 
    \begin{itemize}
        \item No sparsity problem
        \item Don't need to store all n-grams you've seen.
    \end{itemize}
    \item \textbf{Problems} 
    \begin{itemize}
        \item Fixed window will always be small. Increasing it will increase size of $W$, leading to more problems
        \item No symmetry i.e because of matrix multiplication, each word vector is multiplied only by specified column of wight vectors. So you are learning something specific for each word rather than learning a general function.
        \item Need neural network to process strings of arbitary length
    \end{itemize}
\end{itemize}
\section{RNN}
\begin{itemize}
    \item Recurrent neural network
    \item Any length input sequence
    \item Sequence of hidden states. Each state computed based on previous hidden state and the current input. Also called \textit{timesteps} 
    \item \textbf{Same weight matrix is applied at each step}. This helps learn a general function.  
    \item Hidden state computed as:
    \begin{equation}
    \begin{split}
        h^t = \sigma(W_hh^{t-1} + W_ee^t + b_1)
    \end{split}
    \end{equation}
    \item Initial hidden state can be either vector of 0's or any arbirary vector.
    \item Final output will have softmax layer. Output size will be of size vocab $V$
    \item \textbf{Advantages}
    \begin{itemize}
        \item Any length of input
        \item Computation at $t$ can use info from previous steps
        \item Symmetry in weights application i.e learns a general function
    \end{itemize}
    \item \textbf{Disadvantages} 
    \begin{itemize}
        \item Recurrent computation is slow
        \item In practice, info from many steps back is difficult to retain
    \end{itemize}
    \item Training
        \begin{itemize}
            \item Obtain big corpus
            \item  Feed into RNN-LM. Compute $\hat{y}$ for every step $y$ 
            \item Loss function is cross entropy between prediced next word and the true next word $y^t$
                $J(\theta) = -log(\hat{y}^t_{x_{t+1}})$
        \end{itemize}
    \item Computing gradient and loss over entire corpus is expensive. In practice, use input as a sentece or collection of senteces
    \item Apply \textit{Stochastic gradient descent} and compute everything in batches
    \item Backpro in RNN will be the sum of the gradient wrt each time it appears. Words using multivariable chain rule
    \begin{equation}
    \begin{split}
        \frac{\delta J^t}{\delta W_h} = \sum_{u=1}^{t} \delta J^\frac{t}{\delta W_h}
    \end{split}
    \end{equation}
    \item Accumulate the gradients at each timestep. Also called \textit{backpropogation through time} 
    \item Generating text will be the same as repeated sampling used in the n-gram model
\end{itemize}
\subsection{Evaluation LM}
\begin{itemize}
    \item \textbf{Perplexity}: Inverse probability of the corpus given the language model  
    \begin{equation}
    \begin{split}
        perplexity = \prod_{t=1}^T \bigg( \frac{1}{P_{LM}(x^{t+1}|x^t,...,x^1}\bigg )^{\frac{1}{T}}
        perplexity = exp(J(\theta))
    \end{split}
    \end{equation}
\end{itemize}
\section{Recap}
\begin{itemize}
    \item LM predicts next word
    \item RNN Seq input of any length, apply same weights and produce output
    \item RNN to build LM
    \item Example: RNN for POS tagging task
    \item RNN can also be used as a general purpose encoder model. Can be used for machine translation, question answering, etc.
    \item \textit{Vanilla RNN} = RNN in this lecture 
    \item Multilayer RNN possible
\end{itemize}
\end{document}
