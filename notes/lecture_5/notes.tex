\documentclass[a4paper]{article}

\usepackage{amsmath, graphicx, float, blindtext} % for dummy text
\graphicspath{ {./images/} }
\title{Dependency Parsing}
\author{Shubham Gupta}

\begin{document}
\maketitle
\section{Introduction}
\subsection{Phrase structure}
\begin{itemize}
    \item words
    \item phrases
    \item Bigger phrases(sentences lol)
    \item Also called Context Free Grammer(CFG). Building structure for the language using Noun(N), Verb(V), Verb Phrase(VP), etc.
\end{itemize}
\subsection{Dependency structure}
\begin{itemize}
    \item Words depend on other words.
    \item Why do we need all this?
        \begin{itemize}
            \item Understand language structure
            \item Need to know wat is connected is to what.
        \end{itemize}
\end{itemize}

\section{Ambiguities}
\subsection{Prepositional phrase attachment ambiguity}
\begin{itemize}
    \item Occurs when you have PP before a Noun or Noun Phrase or a Verb.
\end{itemize}
\subsection{Coordination phrase ambiguity}
\begin{itemize}
    \item Example: Doctor: No heart, cognitive issues.
\end{itemize}
\subsection{Adjectival Modifier Ambiguity}
\begin{itemize}
    \item Example: Students get first hand job experience(looool)
\end{itemize}
\subsection{Verb Phrase attachment ambiguity}
\begin{itemize}
    \item Example: Mutilated body washes up rio beach to be used for olympics
\end{itemize}
\section{Dependency Grammar}
\begin{itemize}
    \item Syntactic structure between lexical terms (normally arrows called \textit{depedencies}) 
    \item Arrows have the relationship type between the two words. They connect \textit{head} and the \textit{dependant} of the dependency
\end{itemize}
\section{Depedency conditioning preferences}
\begin{itemize}
    \item Bilexical affinities $\implies$ discussion to issues is possible
    \item Depedency distance $\implies$ mostly with nearby words
    \item Intervening material $\implies$ 
    \item Valency of heads
\end{itemize}
\subsection{How to do it?}
\begin{itemize}
    \item Choose for each word what the other word(including the ROOT)is it a dependant of
    \item Only one word is a dependent of the ROOT
    \item No cycles i.e A -> B, B -> A
\end{itemize}
\section{How to do dependency parsing}
\begin{itemize}
    \item \textbf{Dynamic Programming}  $O(n^3)$ 
    \item \textbf{Graph algorithms} Create a minimum spanning tree 
    \item \textbf{Constraint satisfaction} Apply constraints to edges. Maybe this could be done with constraint programming? 
    \item \textbf{Transition-based or deterministic dependecy parsing}: Use greedy algo. Most used.
\end{itemize}
\subsection{Greedy based depedency parsing}
\begin{itemize}
    \item Sequence of bottom-up actions
    \item Has the following parts:
    \begin{itemize}
        \item stack $\sigma$ written with top to the right. Starts with root symbol
        \item buffer $ \beta$ written with top to the left. Starts with input sentence
        \item Finish if $\sigma$ = $\beta$
    \end{itemize}
    \item For this algo, you would need to explore all possible paths before finding the optimal path. Not very efficient
\end{itemize}
\subsection{MaltParser}
\begin{itemize}
    \item Use maseen learningggg lol
    \item Use a discrimative classifier to predict next action to take.
    \begin{itemize}
        \item Max 3 untyped choices: |R| x 2 + 1
        \item Features: top of stack word, POS, first buffer word, POS
    \end{itemize}
    \item Super fast linear parsing with mad performance
    \item Just below current SOTA
\end{itemize}
\subsection{Evaluation of depedency parsers}
\begin{itemize}
    \item Compare with labelled data
\end{itemize}
\section{Neural depedency parsers}
\begin{itemize}
    \item Problems with previous methods
    \begin{itemize}
        \item Conventional method has sparse matrices
        \item incomplete
        \item Expensive
    \end{itemize}
    \item Instead of specifying the sparse matrix, train neural network to learn the representation automatically
    \item Represent each word as a vector of dimension $d$
    \item Also label POS and dependency labels as vectors of dimension $d$
    \item greater accuracy and speed comparsed to maltparser
\end{itemize}
\end{document}
