\documentclass[a4paper]{article}

\usepackage{amsmath, blindtext, float, graphicx, hyperref}
\graphicspath{ {./images/} }
\title{Vanishing Gradients and Fancy RNNs}
\author{Shubham Gupta}

\begin{document}
\maketitle
\section{Introduction}
\begin{itemize}
    \item Learn about problems with RNN
    \item More RNN variants
\end{itemize}

\section{Vanishing gradient problem}
\begin{itemize}
    \item Occurs in RNN
    \item Small gradient in each step reduces the overall gradient signal as it backpropogates further.
    \item Why is it a problem?
    \begin{itemize}
        \item Gradient signal from faraway is lost because it's much smaller than gradient signal from closeby.
        \item Model weights are only updated with respect to \textit{near effects}, not long term effects.
        \item \textit{TLDR}: Model will not learn the parameters well and hence will have weak predictablity.
        \item Gradient is the effect of the past on the future.
        \item If it doesnt learn the parameters, then either
        \begin{itemize}
            \item No dependency at t and t+1
            \item Or it learns wrong parameters to capture true dep between $t$ and $t+1$
        \end{itemize}
        \item Syntactic recency: Pays attention to syntax of sentence i.e longer language dependency
        \item Sequential recency: Pays attention to things that only happen recently
        \item Due to vanishing gradient problem, RNN learns sequency recency more.
    \end{itemize}
\end{itemize}
\section{Exploding gradients}
\begin{itemize}
    \item Gradient too big $\implies$ SGD update too big
        $ \theta^{new} = \theta^{old} - \alpha \delta_{\theta}J(\theta $
    \item Solution: \textbf{Gradient clipping} 
    \item If norm of gradient is above threshold, normalize gradient before applying SGD update
    \item Normalize gradient by setting max and min thresholds. This will prevent gradient from chaging drastically, thereby avoiding exploding gradiesnts problem.
\end{itemize}
\subsection{Fix vanishing gradients problem}
\begin{itemize}
    \item Seperate memory for longer dependencies
    \item Solution: \textbf{LSTM}  
    \item At step $t$, there is hidden state $h^t$ and cell state $c^t$
     \item Can erase, read and write cell state
    \item Gates control whether they will write, read, etc.
    \item Gates are also vectors
    \item Gates are dynamic. Diff on each step $t$.
    \item Gates are as follows:
    \begin{itemize}
        \item \textit{Forget gate}: $\sigma(W_fh^{t-1} + U_fx^t + b_f)$
        \item \textit{Input gate}: $\sigma(W_ih^{t-1} + U_fx^t + b_i)$
        \item \textit{Output gate}: $\sigma(W_oh^{t-1} + U_fx^t + b_o)$
    \end{itemize}
    \item New cell content: $c^t = tanh(W_ch^{t-1} + U_fx^t + b_c)$
    \item Forget some info using the forget gate
    \item Hidden state read output from some cell
    \item Solves vanishing gradient problem
    \begin{itemize}
        \item Preserves info over many timesteps. If forget gate is set to remember everything on every $t$, the info in the cell is preserved indefinitely.
        \item Harder for RNN to do the same
    \end{itemize}
\end{itemize}
\section{Gated Recurrent Units}
\begin{itemize}
    \item Keep strengths of LSTM but remove complexity
    \item Input $x^t$ and hidden state $h^t$ (no cell state)
    \item Update gate and reset gate
    \begin{equation}
    \begin{split}
        u^t = \sigma(W_uh^{t-1} + U_ux^t + b_u)
        r^t = \sigma(W_rh^{t-1} + U_rx^t + b_r)
    \end{split}
    \end{equation}
    \item How does it solve vanishing gradient?
    \begin{itemize}
        \item Easier to retain long term info
        \item If $u^t$ is 0, then $h^t$ is kept the same at every step.
    \end{itemize}
\end{itemize}
\section{LSTM vs GRU}
\begin{itemize}
    \item LSTM and GRU most widely used
    \item GRU is \textit{quicker to compute} 
    \item No other pros/cons
    \item LSTM is \textbf{good default choice }  
    \item Start with LSTM, switch to GRU for faster training
\end{itemize}
\section{Gradient problems}
\begin{itemize}
    \item Occurs in almost all deep networks
    \item Solution: Add some direct connections in the network
    \item Example: Residual connections or skip-connections
    \item Makes deep networks easier to train
    \item \textbf{DenseNet}: Directly connect everything to everything 
    \item \textbf{HighwayNet}: Identity connection vs transformation layer is controlled by a dynamic gate
\end{itemize}
\section{Bidirectional RNN}
\begin{itemize}
    \item Take information from L-R AND R-L
    \item Forward and backward RNN. Concatenate both outputs.
    \item Train both RNN together
    \item Can be used \textbf{only if we have the entire input sequence}  
    \item cant be used to do language modelling
\end{itemize}
\section{Multi layer rnn}
\begin{itemize}
    \item RNN are already deep on one dim(unroll timesteps)
    \item Apply multiple RNN
    \item Compute more complex representations
    \item Also called \textbf{Stacked RNN}  
    \item Perform well
    \item Not as deep as normal cnn. Expensive to compute
\end{itemize}
\section{Conclusion}
\begin{itemize}
    \item LSTM are powerful but GRU is faster
    \item Clip your gradients
    \item Use bidirectionality when possible
    \item Multi layer RNN are powerful. Use it with skip connections and lots of compute lol
\end{itemize}
\end{document}
